{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cleans all analytical model outputs and produces all summary statistics and tables reported in the main paper and supplement. It has to be run after the analysis code `OIC-OO v7`, using the same ControlFile and from the same folder; this code will read the output .tab files from the analysis, and will fail if they are not available. See main `Analysis & Graphing` folder for `ReadMe` with explanation of ControlFile format and fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from shutil import copy\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "\n",
    "def clean_final(baserunname, scen, name='final', subset=['2003', '2004', '2005']):\n",
    "    \"\"\"Clean and process main run output tabfile, yielding tabfile with \n",
    "    only variables (no params) and larger timestep; `subset` is used to \n",
    "    identify params in the output tabfile, and should *not* include the \n",
    "    initial time\"\"\"\n",
    "    table = pd.read_csv(f'{baserunname}_{name}_{scen}.tab', sep='\\t', \n",
    "                        index_col=0, error_bad_lines=False)\n",
    "    display(f\"Processing {baserunname}_{name}_{scen}.tab...\")\n",
    "    \n",
    "    # Split table along secondary time row\n",
    "    table1 = table.iloc[:table.index.get_loc('Time'), :]\n",
    "    table2 = table.iloc[table.index.get_loc('Time'):, :].dropna(axis=1, how='all')\n",
    "    \n",
    "    # Convert secondary time row to str, with g-format to drop float decimals from whole years\n",
    "    table2.columns = table2.iloc[0].apply('{:g}'.format).astype('str')\n",
    "    table2 = table2[1:] # Drop secondary time row\n",
    "    table = pd.concat([table1, table2]) # Concat with joint time str axis to align values by time\n",
    "    table.dropna(how='all', subset=subset, inplace=True) # Drop constants\n",
    "    table = table[table.columns[::4]] # Reduce density of time axis to 0.25 years\n",
    "    table.to_csv(f'{baserunname}_{name}_{scen}_vars.tab', sep='\\t')\n",
    "    \n",
    "    \n",
    "def clean_sens(baserunname, scen, fitlist, name='sens', repvars=['SimVar'], dropvars=['RepErrRaw']):\n",
    "    \"\"\"Clean and process sensitivity output tabfile, yielding cleaned \n",
    "    tabfile '_clean' and `repvars`-only tabfile '_fits' from `fitlist`, \n",
    "    removing `dropvars`\"\"\"\n",
    "    \n",
    "    # Clean sensitivity data\n",
    "    senstable = pd.read_csv(f'{baserunname}_{name}_{scen}_clean.tab', sep='\\t', index_col=[0,1])\n",
    "    senstable = senstable.reorder_levels(['Var', 'Perc']).sort_index()\n",
    "    \n",
    "    # Filter out data for specified variables, by default RepErrRaw\n",
    "    for var in dropvars:\n",
    "        filt = pd.Series(~senstable.index.levels[0].str.startswith(var), \n",
    "                         index=senstable.index.levels[0])\n",
    "        senstable = senstable[filt[senstable.index.get_level_values('Var')].values]\n",
    "    senstable.to_csv(f'{baserunname}_{name}_{scen}_clean.tab', sep='\\t')\n",
    "    \n",
    "    # Extract sensitivity projection fit-to-data\n",
    "    fits_sens = senstable.loc[[f'{repvar}[{var[0]}]' for repvar in repvars for var in fitlist]]\n",
    "    fits_sens.to_csv(f'{baserunname}_{name}_{scen}_fits.tab', sep='\\t')\n",
    "\n",
    "\n",
    "def insert_sums(tablename, sumlist, sumvars=['SimVar', 'DataVar'], index_col=0):\n",
    "    \"\"\"Calculate summed variables and add to tabfile; sums each var in \n",
    "    `sumvars` for elements specified in `sumlist`\"\"\"\n",
    "    t = pd.read_csv(tablename, sep='\\t', index_col=index_col)\n",
    "    \n",
    "    t_dict = {}\n",
    "    # For each triplet in sumlist, set first elm as sum of other two\n",
    "    for a, b, c in sumlist:\n",
    "        for var in sumvars:\n",
    "            t_dict[f'{var}[{a}]'] = t.loc[f'{var}[{b}]'] + t.loc[f'{var}[{c}]']\n",
    "    \n",
    "    # Compile summed variables\n",
    "    if index_col==0:\n",
    "        t_sums = pd.concat(t_dict, axis=1).T\n",
    "    else:\n",
    "        t_sums = pd.concat(t_dict)\n",
    "    \n",
    "    # Merge back in to main dataframe and export to tabfile\n",
    "    t = pd.concat([t, t_sums]).sort_index()\n",
    "    t.to_csv(tablename, sep='\\t')\n",
    "\n",
    "\n",
    "def calc_gof(resdf, simvar, datavar):\n",
    "    \"\"\"Calculate goodness-of-fit measures for given sim & data vars\"\"\"\n",
    "    # IMPORTANT: cross-screen for missing sim or data values\n",
    "    sim = resdf.loc[simvar].where(resdf.loc[datavar].notna())\n",
    "    dat = resdf.loc[datavar].where(resdf.loc[simvar].notna())\n",
    "    \n",
    "    # Calculate various GOF stats & return each one\n",
    "    error = abs(sim - dat)\n",
    "    maen = error.mean()/dat.mean()\n",
    "    mape = (error/dat).mean()\n",
    "    simstd = np.sqrt((sim ** 2).mean() - sim.mean() ** 2)\n",
    "    datastd = np.sqrt((dat ** 2).mean() - dat.mean() ** 2)\n",
    "    r2 = (sim.corr(dat)) ** 2\n",
    "    mse = (error ** 2).mean()\n",
    "    um = ((sim.mean() - dat.mean()) ** 2/ mse)\n",
    "    us = ((simstd - datastd) ** 2/ mse)\n",
    "    uc = (2 * (1 - sim.corr(dat)) * simstd * datastd / mse)\n",
    "    return maen, mape, r2, mse, um, us, uc\n",
    "    \n",
    "    \n",
    "def get_year_values(table, senstable, var, years, percs, name):\n",
    "    \"\"\"Get value and bounds of specified `var` in `years` as text\"\"\"\n",
    "    vartext = [name + '\\n'] # Initialise with specified name, varname by default\n",
    "\n",
    "    # Iterate through years specified and pull values for each\n",
    "    for year in years:\n",
    "        val = table.loc[var, str(year)]\n",
    "        lower = senstable.loc[var, percs[0]].loc[str(float(year))]\n",
    "        upper = senstable.loc[var, percs[1]].loc[str(float(year))]\n",
    "\n",
    "        vartext.append(f\"{year}\\t{val}\\t{lower}\\t{upper}\\n\")\n",
    "\n",
    "    return vartext\n",
    "\n",
    "\n",
    "def compare_vals(first, second, projvars, projyear, compperc=50.0):\n",
    "    \"\"\"Calculate differences between specified `projvars` in `projyear` \n",
    "    for `first` and `second`, using `first` as reference values\"\"\"\n",
    "    vals = []\n",
    "    for file in first, second:\n",
    "        senstable = pd.read_csv(file, sep='\\t', index_col=[0,1])\n",
    "        senstable = senstable[senstable.columns[::4]]\n",
    "        senstable.columns = senstable.columns.astype(float).astype(int)\n",
    "        vals.append([senstable.loc[var, compperc][projyear] for var in projvars])\n",
    "        del senstable\n",
    "\n",
    "    # Calculate change in values using first as reference point\n",
    "    vals_chg = [(var1-var0)/var0 for var0, var1 in zip(vals[0], vals[1])]\n",
    "    return vals_chg\n",
    "\n",
    "\n",
    "def compile_sens_panel(baserunname, name, key, scen, outvars, projvars, endyear, \n",
    "                       projyear, params=True, dropvars=None):\n",
    "    \"\"\"Compile key outcomes panel for sensitivity analysis, specifying \n",
    "    run for comparison with `name`, `key` and `scen`, key outcome vars \n",
    "    with `outvars` at `endyear` and `projvars` at `projyear`, including \n",
    "    parametric sensitivity if `params` is True (and excluding params \n",
    "    e.g. from loop knockout with `dropvars`)\"\"\"\n",
    "    \n",
    "    # Read in base run for comparison and subset key outcome values\n",
    "    b = pd.read_csv(f'{baserunname}_final_{scen}_vars.tab', sep='\\t', index_col=0)\n",
    "    b = b.loc[outvars + projvars][[endyear, projyear]]\n",
    "\n",
    "    sensoutdict = {}\n",
    "    \n",
    "    # Read in sensitivity run and calculate change in key outcome values vs. base\n",
    "    t = pd.read_csv(f'{baserunname}_{name}_{key}_{scen}_vars.tab', sep='\\t', index_col=0)\n",
    "    for var in outvars:\n",
    "        sensoutdict[var] = (t.loc[var, endyear] - b.loc[var, endyear]) / b.loc[var, endyear]\n",
    "    for var in projvars:\n",
    "        sensoutdict[var] = (t.loc[var, projyear] - b.loc[var, projyear]) / b.loc[var, projyear]\n",
    "\n",
    "    # Read in parameter values and calculate sensitivity\n",
    "    if params:\n",
    "        paramdf = pd.read_csv(f'{baserunname}_{name}_params.tab', sep='\\t', index_col=0)\n",
    "\n",
    "        pt = paramdf[['Value', key]] # Select values for relevant run\n",
    "        if dropvars: # Drop specified params (e.g. knocked-out loops)\n",
    "            pt = pt.drop(dropvars)\n",
    "        \n",
    "        pt = pt[pt['Value'] > 0.0001] # Screen out values below 1e-04\n",
    "        pt['Chg'] = (pt[key] - pt['Value']) / pt['Value']\n",
    "        sensoutdict['Med elasticity'] = abs(pt['Chg']).median()\n",
    "        sensoutdict['Max elasticity'] = max(pt['Chg'].min(), pt['Chg'].max(), key=abs)\n",
    "    else: # Or specify zero elasticity\n",
    "        sensoutdict['Med elasticity'] = 0\n",
    "        sensoutdict['Max elasticity'] = 0\n",
    "\n",
    "    # Subset sensitivity run results\n",
    "    t = t.loc[:, :endyear]\n",
    "\n",
    "    # Calculate goodness-of-fit statistics\n",
    "    t = t[t.columns[::4]] # Subset to each year instead of 0.25 years\n",
    "    gofs = [[*calc_gof(t, f'SimVar[{elm[0]}]', f'DataVar[{elm[0]}]')] for elm in fitlist]\n",
    "\n",
    "    gofdf = pd.DataFrame(gofs, index=[elm[0] for elm in fitlist], \n",
    "                         columns=['MAEN', 'MAPE', 'R2', 'MSE', 'Um', 'Us', 'Uc'])\n",
    "    gofdf.loc['Avg'] = gofdf.mean()\n",
    "    gofdf.to_csv(f'{baserunname}_{name}_{key}_GOF.tab', sep='\\t')\n",
    "\n",
    "    # Pull relevant GOF statistics from GOF stats dataframe\n",
    "    sensoutdict['Avg MAEN'] = gofdf.loc['Avg', 'MAEN']\n",
    "    sensoutdict['Max MAEN'] = gofdf['MAEN'].max()\n",
    "    \n",
    "    # Return series with each key outcome for the panel\n",
    "    return pd.Series(sensoutdict)\n",
    "\n",
    "\n",
    "def strbds_from_perc(perc):\n",
    "    \"\"\"Return lower & upper bounds that define `perc` CrI as strings\"\"\"\n",
    "    if perc > 1: # If perc specified as percentage (not decimal)\n",
    "        return [str(round((0.5 - perc/200), 3)), str(round((0.5 + perc/200), 3))]\n",
    "    else: # If perc specified as decimal, not 100%\n",
    "        return [str(round((0.5 - perc/2), 3)), str(round((0.5 + perc/2), 3))]\n",
    "\n",
    "\n",
    "def get_value(file, varname):\n",
    "    \"\"\"General purpose function for reading values from .mdl, .out, etc. \n",
    "    files; returns value matching `varname` in a 'var = val' syntax\"\"\"\n",
    "    varregex = regex.compile(r'(?<=([^\\w ]|\\n)\\s?' + regex.escape(varname)\n",
    "                             + r'\\s*=)\\s*-?(?:\\d*)(\\.\\d*)?([eE][+\\-]?\\d+)?')\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        filetext = f.read()\n",
    "        value = float((regex.search(varregex, filetext))[0])\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read specified controlfile and unpack into variables\n",
    "controlfilename = input(\"Enter control file name (with extension):\")\n",
    "cf = json.load(open(controlfilename, 'r'))\n",
    "\n",
    "for k,v in cf.items():\n",
    "    exec(k + '=v')\n",
    "\n",
    "for setting in [analysissettings]:\n",
    "    for k, v in setting.items():\n",
    "        exec(k + '=v')\n",
    "\n",
    "# Initialise base working directory\n",
    "os.chdir(f\"{baserunname}_IterCal\")\n",
    "basedir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA FILE PREPARATION AND CLEANING #####\n",
    "\n",
    "os.chdir(basedir)\n",
    "os.makedirs('./Results', exist_ok=True)\n",
    "os.chdir('./Results')\n",
    "\n",
    "# Copy over all necessary files from other directories\n",
    "copy(f'../{baserunname}_main_fits.tab', './')\n",
    "copy(f'../{baserunname}_params.tab', './')\n",
    "for cin in (basescens + scenariolist):\n",
    "    copy(f'../Scenarios/{baserunname}_final_{cin[:-4]}.tab', './')\n",
    "    copy(f'../Scenarios/{baserunname}_sens_{cin[:-4]}_clean.tab', './')\n",
    "\n",
    "for cin in basescens[0:2]:\n",
    "    for proj in proj_subs:\n",
    "        copy(f'../Scenarios/{baserunname}_final_{cin[:-4]}{proj}.tab', './')\n",
    "        copy(f'../Scenarios/{baserunname}_sens_{cin[:-4]}{proj}_clean.tab', './')\n",
    "\n",
    "# Clean & process projection & policy scenario results\n",
    "for scen in [cin[:-4] for cin in (basescens + scenariolist)]:\n",
    "    clean_final(baserunname, scen)\n",
    "    clean_sens(baserunname, scen, fitlist)\n",
    "\n",
    "    \n",
    "for scen in [(cin[:-4] + pol[:-4]) for cin in basescens for pol in policylist]:\n",
    "    clean_final(baserunname, scen)\n",
    "    \n",
    "# Clean & process alternative data condition analysis results\n",
    "for scen in [f'{cin[:-4]}_{basescens[0][:-4]}' for cin in testlist]:\n",
    "    copy(f'../Sensitivity/{baserunname}_test_{scen}.tab', './')\n",
    "    copy(f'../Sensitivity/{baserunname}_test_params.tab', './')\n",
    "    clean_final(baserunname, scen, name='test')\n",
    "\n",
    "# Clean & process loop knockout sensitivity results\n",
    "copy(f'../Sensitivity/{baserunname}_lk_params.tab', './')\n",
    "for key in lkdict.keys():\n",
    "    for name in ['lk', 'lk_run']:\n",
    "        copy(f'../Sensitivity/{baserunname}_{name}_{key}_{basescens[0][:-4]}.tab', './')\n",
    "        clean_final(baserunname, f'{key}_{basescens[0][:-4]}', name=name)\n",
    "\n",
    "# Clean & process parametric assumptions sensitivity results\n",
    "sensdict = dict([[''.join([w[0] for w in regex.findall(r\"[\\w']+\", var)]), var] \n",
    "                 for var in sensvars])\n",
    "copy(f'../Sensitivity/{baserunname}_assm_params.tab', './')\n",
    "for key in sensdict.keys():\n",
    "    for sfx in ['_L', '_H']:\n",
    "        copy(f'../Sensitivity/{baserunname}_assm_{key}{sfx}_{basescens[0][:-4]}.tab', './')\n",
    "        clean_final(baserunname, f'{key}{sfx}_{basescens[0][:-4]}', name='assm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE AGGREGATED VARIABLES, STD ERRS & GOF STATISICS #####\n",
    "\n",
    "fitdict = dict(fitlist)\n",
    "\n",
    "# Specify aggregate variables to calculate w/ labels\n",
    "sumlist = [('ROUT', 'ROUD', 'ROUH'), ('InRT', 'InRM', 'InRD'), ('ODRT', 'ODRB', 'ODSB')]\n",
    "fitdict['ROUT'] = 'Total Rx opioid use disorder'\n",
    "fitdict['InRT'] = 'Total Rx misuse initiation'\n",
    "fitdict['ODRT'] = 'Overdose deaths (Rx + Rx synthetics)'\n",
    "\n",
    "# Calculate aggregates for various results tabfiles\n",
    "insert_sums(f'{baserunname}_main_fits.tab', sumlist)\n",
    "for cin in basescens + scenariolist:\n",
    "    insert_sums(f'{baserunname}_final_{cin[:-4]}_vars.tab', sumlist, \n",
    "                sumvars=['SimVar', 'DataVar', 'RepVar'])\n",
    "    insert_sums(f'{baserunname}_sens_{cin[:-4]}_clean.tab', sumlist, \n",
    "                sumvars=['SimVar', 'RepVar'], index_col=[0,1])\n",
    "\n",
    "# Read and append standard error terms where available\n",
    "tssd = pd.read_excel('../../Time series standard deviations.xlsx', \n",
    "                     sheet_name='Summary', index_col=[0,1], header=1)\n",
    "\n",
    "stderrdict = {} # Initialise container for stderrs\n",
    "stderrdict['DataErr[InRT]'] = tssd.loc[\n",
    "    ('Total Rx misuse initiation SAMHSA', 'Standard Error of Weighted Mean')]\n",
    "stderrdict['DataErr[InHT]'] = tssd.loc[('Total heroin initiation SAMHSA', 'RAND Multiplied SE')]\n",
    "stderrdict['DataErr[ROUT]'] = (\n",
    "    tssd.loc[('Rx OUD no PY heroin NSDUH', 'Standard Error of Weighted Mean')] \n",
    "    + tssd.loc[('Rx OUD + H NSDUH RAND', 'RAND Multiplied SE')])\n",
    "stderrdict['DataErr[HUD]'] = tssd.loc[('HUD NSDUH RAND', 'RAND Multiplied SE')]\n",
    "\n",
    "stderrs = pd.concat(stderrdict, axis=1).T # Concatenate stderr series and transpose\n",
    "stderrs.columns = stderrs.columns.astype('str')\n",
    "\n",
    "fits = pd.read_csv(f'{baserunname}_main_fits.tab', sep='\\t', index_col=0)\n",
    "fits = pd.concat([fits, stderrs])\n",
    "\n",
    "fits.to_csv(f'{baserunname}_main_fits.tab', sep='\\t')\n",
    "\n",
    "# Calculate goodness-of-fit statistics\n",
    "fits = fits[fits.columns[::4]]\n",
    "gofs = [[*calc_gof(fits, f'SimVar[{elm}]', f'DataVar[{elm}]')] for elm in fitdict.keys()]\n",
    "\n",
    "gofdf = pd.DataFrame(gofs, index=fitdict.values(), \n",
    "                     columns=['MAEN', 'MAPE', 'R2', 'MSE', 'Um', 'Us', 'Uc'])\n",
    "gofdf.loc['Average'] = gofdf.iloc[0:-3].mean() # Leave out calculated aggregates from average\n",
    "gofdf.to_csv(f'{baserunname}_GOF.tab', sep='\\t')\n",
    "display(gofdf)\n",
    "\n",
    "# Create MCMC sample correlation matrix\n",
    "mcsample = pd.read_csv(f'../{baserunname}_main_MC_MCMC_sample_frac.tab', sep='\\t')\n",
    "mcsample.dropna(how='all', axis=1, inplace=True)\n",
    "corrs = mcsample.corr()\n",
    "corrs.to_csv(f'{baserunname}_MC_sample_correlations.tab', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMPILE AND EXPORT INPUT VALUES AND SELECTED YEAR-BY-YEAR VALUES #####\n",
    "\n",
    "# Assemble input time series projection values\n",
    "t = pd.read_csv(f'{baserunname}_final_{basescens[0][:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "p = pd.read_csv(f'{baserunname}_final_{scenariolist[0][:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "n = pd.read_csv(f'{baserunname}_final_{scenariolist[1][:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "\n",
    "inputslist = [f'Input\\t{endyear}\\t{projyear}\\tOptimistic\\tPessimistic\\n'] # Initialise with column labels\n",
    "for proj in proj_subs:\n",
    "    endval = t.loc[f'Projection output data[{proj}]', str(endyear)]\n",
    "    projval = t.loc[f'Projection output data[{proj}]', str(projyear)]\n",
    "    posval = p.loc[f'Projection output data[{proj}]', str(projyear)]\n",
    "    negval = n.loc[f'Projection output data[{proj}]', str(projyear)]\n",
    "    inputslist.append(f'{proj}\\t{endval}\\t{projval}\\t{posval}\\t{negval}\\n')\n",
    "    \n",
    "with open(f'{baserunname}_inputs.tab', 'w') as f:\n",
    "    f.writelines(inputslist)\n",
    "del t\n",
    "\n",
    "# Compile yearvals output for specified variables and years from sensitivity projections\n",
    "t = pd.read_csv(f'{baserunname}_final_{basescens[0][:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "s = pd.read_csv(f'{baserunname}_sens_{basescens[0][:-4]}_clean.tab', sep='\\t', index_col=[0,1])\n",
    "\n",
    "vartext = [f'Year\\tVal\\t{yv_percs[0]}\\t{yv_percs[1]}\\n'] # Initialise with column labels\n",
    "for var in yearvals:\n",
    "    vartext.extend(get_year_values(t, s, var, years, yv_percs, var))\n",
    "\n",
    "# Add prior values\n",
    "for prior in priorlist:\n",
    "    vartext.extend(get_year_values(t, s, f'SimPrior[{prior[0]}]', [prior[1]], yv_percs, prior[2]))\n",
    "\n",
    "# Get projection end values for each main scenario\n",
    "for cin in (basescens[:1] + scenariolist[0:2]):\n",
    "    t = pd.read_csv(f'{baserunname}_final_{cin[:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "    s = pd.read_csv(f'{baserunname}_sens_{cin[:-4]}_clean.tab', sep='\\t', index_col=[0,1])\n",
    "    \n",
    "    for var in projvars:\n",
    "        vartext.extend(get_year_values(t, s, var, [str(projyear)], yv_percs, cin[:-4] + ' ' + var))\n",
    "\n",
    "with open(f'{baserunname}_yearvals.tab', 'w') as f:\n",
    "    f.writelines(vartext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ALTERNATIVE PROJECTION ASSUMPTIONS SENSITIVITY ANALYSIS #####\n",
    "\n",
    "dflist = [] # Initialise empty container\n",
    "\n",
    "# Iterate through using each basescen as reference point\n",
    "for cin in basescens[0:2]:\n",
    "    first = f'{baserunname}_sens_{cin[:-4]}_clean.tab' # Specify reference scenario\n",
    "\n",
    "    # Calculate comparison for each projection assumption\n",
    "    vals_chgs = []\n",
    "    for proj in proj_subs:\n",
    "        second = f'{baserunname}_sens_{cin[:-4]}{proj}_clean.tab'\n",
    "        vals_chgs.append(compare_vals(first, second, projvars, projyear))\n",
    "    dflist.append(pd.DataFrame(vals_chgs, index=proj_subs, columns=projvars))\n",
    "\n",
    "avgchgdf = (dflist[0] - dflist[1]) / 2 # NOTE: expressed as delta from basescens[0] to [1]\n",
    "dflist.append(avgchgdf)\n",
    "\n",
    "# Assemble and export comparison results\n",
    "cols = [f'{cin} {var}' for cin in ['Base', 'Cnst', 'Avg'] for var in projvars]\n",
    "chgsdf = pd.concat(dflist, axis=1)\n",
    "chgsdf.loc['MAC'] = abs(chgsdf).mean() # Calculate mean absolute change\n",
    "chgsdf.columns = cols\n",
    "chgsdf.to_csv(f'{baserunname}_proj_changes.tab', sep='\\t')\n",
    "\n",
    "chgsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### PRODUCE SUMMARY TABLES FROM POLICY ANALYSIS #####\n",
    "\n",
    "# Process annual and cumulative main results for each scenario and baseline case\n",
    "for cin in basescens:\n",
    "    # Read in baseline results\n",
    "    b = pd.read_csv(f'{baserunname}_final_{cin[:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "    resdict = {'Baseline': b.loc[annvars]}\n",
    "    curdict = {'Baseline': b.loc[projvars]}\n",
    "    cumdf = pd.DataFrame(columns=projvars) # Initialise container dataframe\n",
    "    cumdf.loc['Baseline'] = [b.loc[var, str(projyear)] - b.loc[var, str(polstart)] \n",
    "                             for var in projvars] # Re-zero to polstart year value\n",
    "    del b # Clear results to free up memory\n",
    "\n",
    "    for pol in policylist:\n",
    "        # Read in results for each scenario\n",
    "        scen = cin[:-4] + pol[:-4]\n",
    "        t = pd.read_csv(f'{baserunname}_final_{scen}_vars.tab', sep='\\t', index_col=0)\n",
    "        resdict[pol[:-4]] = t.loc[annvars]\n",
    "        curdict[pol[:-4]] = t.loc[projvars]\n",
    "        cumdf.loc[pol[:-4]] = [t.loc[var, str(projyear)] - t.loc[var, str(polstart)] \n",
    "                               for var in projvars] # Re-zero to polstart year value\n",
    "        \n",
    "    # Compile cumulative and annual results dataframes\n",
    "    resdf = pd.concat(resdict, names=['Scenario', 'Var'])\n",
    "    curdf = pd.concat(curdict, names=['Scenario', 'Var'])\n",
    "    curdf = curdf.subtract(curdf[str(polstart)], axis=0)\n",
    "    resdf = pd.concat([resdf, curdf])\n",
    "    resdf = resdf.reorder_levels(['Var', 'Scenario']).sort_index()\n",
    "    resdf = resdf.loc[:, str(polstart):]\n",
    "    \n",
    "    # Calculate % changes\n",
    "    chgdict = {}\n",
    "    for var in annvars: # Calculate and append for annual results\n",
    "        chgvar = f'% change in {var}'\n",
    "        chgdict[chgvar] = ((resdf.loc[var] - resdf.loc[(var, 'Baseline')]) \n",
    "                           / resdf.loc[(var, 'Baseline')]) # Calculate % change from baseline\n",
    "    chgdf = pd.concat(chgdict)\n",
    "    resdf = resdf.append(chgdf)\n",
    "    \n",
    "    for var in projvars: # Calculate and append for cumulative results\n",
    "        chgvar = f'% change in {var}'\n",
    "        cumdf[chgvar] = (cumdf[var] - cumdf.loc['Baseline', var])/ cumdf.loc['Baseline', var]\n",
    "\n",
    "    # Rename scenarios with specified labels\n",
    "    resdf.rename(polnames, inplace=True)\n",
    "    cumdf.rename(polnames, inplace=True)\n",
    "\n",
    "    resdf.to_csv(f'{baserunname}_{cin[:-4]}_PolRes.tab', sep='\\t')\n",
    "    cumdf.to_csv(f'{baserunname}_{cin[:-4]}_PolTot.tab', sep='\\t')\n",
    "    \n",
    "# Process annual results with CrI quantiles from full sensitivity sample\n",
    "pollist = [f'{basescens[0][:-4]}{cin[:-4]}' for cin in policylist] # Compile list of scenarios\n",
    "\n",
    "poldict = {} # Initialise container for relevant results\n",
    "# Add main and sens results for each scenario to container\n",
    "for scen in pollist:\n",
    "    t = pd.read_csv(f'{baserunname}_final_{scen}_vars.tab', sep='\\t', index_col=0)\n",
    "    s = pd.read_csv(f'{baserunname}_sens_{scen}_clean.tab', sep='\\t', index_col=[0,1])\n",
    "    t.columns = t.columns.astype(float)\n",
    "    s.columns = s.columns.astype(float)\n",
    "    \n",
    "    scendict = {}\n",
    "    for var in polvars: # Add expected values from baserun to sensitivity dataframe\n",
    "        scendict[var] = s.loc[var].sort_values([polstart, styear, endyear])\n",
    "        scendict[var].loc['EV'] = t.loc[var]\n",
    "    poldict[scen] = pd.concat(scendict, keys=polvars, names=['Var', 'Run'])\n",
    "    del s, t # Clear results to free up memory\n",
    "\n",
    "# Compile new dataframe of scenario results with full sample\n",
    "projtable = pd.concat(poldict, names=['Scen', 'Var', 'Run'])\n",
    "projtable.to_csv(f'{baserunname}_polprojraw.tab', sep='\\t')\n",
    "\n",
    "p = projtable.loc[:, polstart:] # Subset results to relevant years\n",
    "p.index = p.index.droplevel('Run')\n",
    "\n",
    "# Calculate annual value quantiles at each time step based on full sample\n",
    "polprojdict = {}\n",
    "for scen in pollist:\n",
    "    scenpercdict = {}\n",
    "    for var in polvars:\n",
    "        scenpercdict[var] = p.loc[(scen, var)].iloc[:-1].quantile(polquants)\n",
    "        scenpercdict[var].loc['EV'] = p.loc[(scen, var)].iloc[-1]\n",
    "    \n",
    "    polprojdict[scen] = pd.concat(scenpercdict, keys=polvars, names=['Var', 'Perc'])\n",
    "\n",
    "# Calculate % change quantiles at each time step based on full sample\n",
    "polpercdict = {}\n",
    "for scen in pollist[1:]:\n",
    "    # First calculate % change across the entire sample\n",
    "    percs = (p.loc[scen] - p.loc[pollist[0]])/p.loc[pollist[0]]\n",
    "    \n",
    "    # Then take quantiles for the % change value at each time step\n",
    "    scenpercdict = {}\n",
    "    for var in polvars:\n",
    "        scenpercdict[var] = percs.loc[var].iloc[:-1].quantile(polquants)\n",
    "        scenpercdict[var].loc['EV'] = percs.loc[var].iloc[-1]\n",
    "    \n",
    "    polpercdict[scen] = pd.concat(scenpercdict, keys=polvars, names=['Var', 'Perc'])\n",
    "\n",
    "# Rename scenarios with specified labels\n",
    "polrenames = dict([[scen, polnames[cin[:-4]]] for scen, cin in zip(pollist, policylist)])\n",
    "\n",
    "# Save compiled tables of annual value and % change quantiles\n",
    "polprojtable = pd.concat(polprojdict, names=['Scen', 'Var', 'Perc'])\n",
    "polprojtable.rename(index=polrenames, level=0, inplace=True)\n",
    "polprojtable.to_csv(f'{baserunname}_polprojann.tab', sep='\\t')\n",
    "\n",
    "polperctable = pd.concat(polpercdict, names=['Scen', 'Var', 'Perc'])\n",
    "polperctable.rename(index=polrenames, level=0, inplace=True)\n",
    "polperctable.to_csv(f'{baserunname}_polprojperc.tab', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TEMPORARY #####\n",
    "##### PAIRWISE POLICY SCENARIO COMBINATIONS ######\n",
    "os.chdir(basedir)\n",
    "os.chdir('./Results')\n",
    "\n",
    "pollist = [\"BupProvBarriers.cin\", \"BupProv.cin\", \"DevOUD.cin\", \"DivRxInit.cin\", \n",
    "           \"FentODRisk.cin\", \"HInit.cin\", \"NxKits.cin\", \"OwnRxInit.cin\", \n",
    "           \"PeerRecovery.cin\",  \"RxRate.cin\", \"ReturntoOUD.cin\"]\n",
    "pollist10 = [\"BupProvBarriers10.cin\", \"BupProv10.cin\", \"DevOUD10.cin\", \"DivRxInit10.cin\", \n",
    "             \"FentODRisk10.cin\", \"HInit10.cin\", \"NxKits10.cin\", \"OwnRxInit10.cin\", \n",
    "             \"PeerRecovery10.cin\", \"RxRate10.cin\", \"ReturntoOUD10.cin\"]\n",
    "pollist50 = [\"BupProvBarriers50.cin\", \"BupProv50.cin\", \"DevOUD50.cin\", \"DivRxInit50.cin\", \n",
    "             \"FentODRisk50.cin\", \"HInit50.cin\", \"NxKits50.cin\", \"OwnRxInit50.cin\", \n",
    "             \"PeerRecovery50.cin\", \"RxRate50.cin\", \"ReturntoOUD50.cin\"]\n",
    "\n",
    "for cin in basescens[0:1]:\n",
    "    for pols in [pollist, pollist10, pollist50]:\n",
    "        for i, pol in enumerate(pols[:-1]):\n",
    "            for pol2 in pols[i+1:]:\n",
    "                scen = f'{cin[:-4]}{pol[:-4]}{pol2[:-4]}'\n",
    "                clean_final(baserunname, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process annual and cumulative main results for each scenario and baseline case\n",
    "for cin in basescens[0:1]:\n",
    "    # Read in baseline results\n",
    "    b = pd.read_csv(f'{baserunname}_final_{cin[:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "    resdict = {'Baseline': b.loc[annvars]}\n",
    "    curdict = {'Baseline': b.loc[projvars]}\n",
    "    cumdf = pd.DataFrame(columns=projvars) # Initialise container dataframe\n",
    "    cumdf.loc['Baseline'] = [b.loc[var, str(projyear)] - b.loc[var, str(polstart)] \n",
    "                             for var in projvars] # Re-zero to polstart year value\n",
    "    del b # Clear results to free up memory\n",
    "\n",
    "    for pols in [pollist, pollist10, pollist50]:\n",
    "        for i, pol in enumerate(pols[:-1]):\n",
    "            for pol2 in pols[i+1:]:\n",
    "                scen = f'{cin[:-4]}{pol[:-4]}{pol2[:-4]}'\n",
    "                t = pd.read_csv(f'{baserunname}_final_{scen}_vars.tab', sep='\\t', index_col=0)\n",
    "                resdict[f'{pol[:-4]} {pol2[:-4]}'] = t.loc[annvars]\n",
    "                curdict[f'{pol[:-4]} {pol2[:-4]}'] = t.loc[projvars]\n",
    "                cumdf.loc[f'{pol[:-4]} {pol2[:-4]}'] = [\n",
    "                    t.loc[var, str(projyear)] - t.loc[var, str(polstart)] for var in projvars]\n",
    "\n",
    "    # Compile cumulative and annual results dataframes\n",
    "    resdf = pd.concat(resdict, names=['Scenario', 'Var'])\n",
    "    curdf = pd.concat(curdict, names=['Scenario', 'Var'])\n",
    "    curdf = curdf.subtract(curdf[str(polstart)], axis=0)\n",
    "    resdf = pd.concat([resdf, curdf])\n",
    "    resdf = resdf.reorder_levels(['Var', 'Scenario']).sort_index()\n",
    "    resdf = resdf.loc[:, str(polstart):]\n",
    "    \n",
    "    # Calculate % changes\n",
    "    chgdict = {}\n",
    "    for var in annvars: # Calculate and append for annual results\n",
    "        chgvar = f'% change in {var}'\n",
    "        chgdict[chgvar] = ((resdf.loc[var] - resdf.loc[(var, 'Baseline')]) \n",
    "                           / resdf.loc[(var, 'Baseline')]) # Calculate % change from baseline\n",
    "    chgdf = pd.concat(chgdict)\n",
    "    resdf = resdf.append(chgdf)\n",
    "    \n",
    "    for var in projvars: # Calculate and append for cumulative results\n",
    "        chgvar = f'% change in {var}'\n",
    "        cumdf[chgvar] = (cumdf[var] - cumdf.loc['Baseline', var])/ cumdf.loc['Baseline', var]\n",
    "\n",
    "    # Rename scenarios with specified labels\n",
    "#     resdf.rename(polnames, inplace=True)\n",
    "#     cumdf.rename(polnames, inplace=True)\n",
    "\n",
    "    resdf.to_csv(f'{baserunname}_Pairwise_PolRes.tab', sep='\\t')\n",
    "    cumdf.to_csv(f'{baserunname}_Pairwise_PolTot.tab', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### LOOP KNOCKOUT ANALYSIS PANEL #####\n",
    "\n",
    "# Set up labels for loop knockout keys\n",
    "lknamedict = {'av': 'Availability', 'pr': 'Perceived risk', 'si': 'Social influence'}\n",
    "\n",
    "lkdfdict = {} # Initiatlise container for results\n",
    "\n",
    "# Iterate through loop knockout keys and compile deactivated and re-estimated results from each\n",
    "for key in lkdict.keys():\n",
    "    lkdfdict['Deactivated ' + lknamedict[key]] = compile_sens_panel(\n",
    "        baserunname, 'lk_run', key, basescens[0][:-4], outvars, projvars, \n",
    "        str(endyear), str(projyear), dropvars=lkdict[key], params=False)\n",
    "    lkdfdict['Recalibrated w/o ' + lknamedict[key]] = compile_sens_panel(\n",
    "        baserunname, 'lk', key, basescens[0][:-4], outvars, projvars, \n",
    "        str(endyear), str(projyear), dropvars=lkdict[key])\n",
    "\n",
    "# Compile and export results\n",
    "lkdf = pd.concat(lkdfdict, axis=1).T\n",
    "lkdf.to_csv(f'{baserunname}_lk_sens.tab', sep='\\t')\n",
    "lkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### PARAMETRIC ASSUMPTIONS SENSITIVITY ANALYSIS PANEL #####\n",
    "\n",
    "# Compile runnames from variable names in sensvars\n",
    "sensdict = dict([[''.join([w[0] for w in regex.findall(r\"[\\w']+\", var)]), var] \n",
    "                 for var in sensvars])\n",
    "\n",
    "assmdfdict = {} # Initiatlise container for results\n",
    "\n",
    "# Iterate through sensvars names and compile results from each\n",
    "for key in sensdict.keys():\n",
    "    # Compile high and low scenario results panels\n",
    "    high = compile_sens_panel(baserunname, 'assm', f'{key}_L', basescens[0][:-4], outvars, \n",
    "                              projvars, str(endyear), str(projyear))\n",
    "    low = compile_sens_panel(baserunname, 'assm', f'{key}_H', basescens[0][:-4], outvars, \n",
    "                             projvars, str(endyear), str(projyear))\n",
    "    \n",
    "    # Concatenate and take average\n",
    "    var = pd.concat({'H': high, 'L': low}, axis=1)\n",
    "    var['avg'] = (abs(var['H']) + abs(var['L'])) / 2 * np.sign(var['H']) # Take sign from H change\n",
    "    var['avg'].iloc[0:6] = var['avg'].iloc[0:6] / sensrange # Convert to elasticity\n",
    "    assmdfdict[sensdict[key]] = var['avg']\n",
    "\n",
    "# Compile and export results\n",
    "assmdf = pd.concat(assmdfdict, axis=1).T\n",
    "assmdf.to_csv(f'{baserunname}_assm_sens.tab', sep='\\t')\n",
    "assmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ALTERNATIVE DATA CONDITIONS SENSITIVITY ANALYSIS PANEL #####\n",
    "\n",
    "# Set up labels for alternative data conditions\n",
    "aldtnamedict = {'Data2019': 'Excluding 2020 data'}\n",
    "\n",
    "aldtdfdict = {} # Initiatlise container for results\n",
    "\n",
    "# Iterate through sensvars names and compile results from each\n",
    "for key, val in aldtnamedict.items():\n",
    "    # Compile high and low scenario results panels\n",
    "    aldtdfdict[val] = compile_sens_panel(baserunname, 'test', key, basescens[0][:-4], \n",
    "                                         outvars, projvars, str(endyear), str(projyear))\n",
    "\n",
    "# Compile and export results\n",
    "aldtdf = pd.concat(aldtdfdict, axis=1).T\n",
    "aldtdf.to_csv(f'{baserunname}_aldt_sens.tab', sep='\\t')\n",
    "aldtdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### HOLDOUT DATA ANALYSIS PROCESSING #####\n",
    "\n",
    "# Clean and process results files\n",
    "clean_final(baserunname, basescens[0][:-4], name='hold_final')\n",
    "clean_sens(baserunname, basescens[0][:-4], fitlist, name='hold_sens', repvars=['RepVar'])\n",
    "\n",
    "# Read in predicted CrI values and actual data values\n",
    "holdfits = pd.read_csv(f'{baserunname}_hold_sens_Base_fits.tab', sep='\\t', index_col=[0,1])\n",
    "holdfits = holdfits.loc[:, :str(float(endyear))]\n",
    "holdfits.drop([f'RepVar[{var}]' for var in hold_excl], inplace=True, level=0)\n",
    "holdfits = holdfits.reorder_levels(['Perc', 'Var']).sort_index()\n",
    "\n",
    "mainfits = pd.read_csv(f'{baserunname}_main_fits.tab', sep='\\t', index_col=[0])\n",
    "datavars = [f'DataVar[{var[0]}]' for var in fitlist]\n",
    "datafits = mainfits.loc[datavars].sort_index()\n",
    "datafits.drop([f'DataVar[{var}]' for var in hold_excl], inplace=True)\n",
    "\n",
    "# Create Boolean df tracking which values are within which percent CrIs\n",
    "for bds in hold_percs:\n",
    "    \n",
    "    # Reindex data values dataframe to use same keys\n",
    "    datafits.columns = holdfits.loc[bds[0]].columns\n",
    "    datafits.index = holdfits.loc[bds[0]].index\n",
    "    \n",
    "    # Compare data values to credible interval limits for entire dataframe\n",
    "    booldf = ((datafits > holdfits.loc[bds[0]]) & (datafits < holdfits.loc[bds[1]]))\n",
    "    booldf.where(datafits.notna(), pd.NA, inplace=True) # Ensure NAs are coded as NA, not False\n",
    "    booldf = booldf[booldf.columns[::4]] # Reduce down to annual resolution\n",
    "    booldf = booldf.loc[:, str(float(holdoutyear+1)):] # Limit to predicted years\n",
    "\n",
    "    # Create 'asterisk' version of Boolean output (for use in tables)\n",
    "    stardf = booldf.copy()\n",
    "    stardf.where(booldf==True, '*', inplace=True)\n",
    "    stardf.mask(booldf==True, '', inplace=True)\n",
    "    \n",
    "    # Create results table of data values and predicted ranges for all vars and prediction years\n",
    "    holddfdict = {}\n",
    "    for yr in booldf.columns:\n",
    "        data, lowers, uppers = (col.astype('int64').apply('{:.2e}'.format) for col in \n",
    "                                [datafits[yr], holdfits.loc[bds[0]][yr], holdfits.loc[bds[1]][yr]])\n",
    "        yrvals = [f'{val}{star} ({lower}-{upper})' for val, star, lower, upper \n",
    "                  in zip(data, stardf[yr], lowers, uppers)]\n",
    "        holddfdict[yr] = pd.Series(yrvals)\n",
    "        \n",
    "    holddf = pd.concat(holddfdict, axis=1)\n",
    "    holddf.index = booldf.index # To assign RepVar names to output\n",
    "    holddf.columns = pd.to_numeric(holddf.columns).astype('int')\n",
    "    holddf.to_csv(f'{baserunname}_holdout.tab', sep='\\t')\n",
    "    booldf.to_csv(f'{baserunname}_holdout_bool.tab', sep='\\t')\n",
    "    \n",
    "    # \n",
    "    predtotal = booldf.size\n",
    "    predright = booldf.sum().sum()\n",
    "    predshort = booldf.drop([f'RepVar[{var}]' for var in hold_drop]).size\n",
    "    predshrgt = booldf.drop([f'RepVar[{var}]' for var in hold_drop]).sum().sum()\n",
    "    \n",
    "    display(booldf)\n",
    "    display(predtotal, predright, predshort, predshrgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### SYNDATA CrI PROCESSING #####\n",
    "\n",
    "# Read in data\n",
    "syndf = pd.read_csv(f'{baserunname}_syndata_results.tab', sep='\\t', index_col=[0, 1])\n",
    "syndf = syndf.reorder_levels(['Perc', 'Run']).sort_index()\n",
    "\n",
    "spdfdict = {} # Initialise container for results\n",
    "\n",
    "# Create Boolean df tracking which values are within which percent CrIs\n",
    "for perc in syn_reppercs:\n",
    "    bds = strbds_from_perc(perc) # Calculate CrI bounds for each percent CrI\n",
    "    spdfdict[perc] = ((syndf.loc['True'] > syndf.loc[bds[0]]) \n",
    "                      & (syndf.loc['True'] < syndf.loc[bds[1]]))\n",
    "\n",
    "# Calculate distance of estimate from median relative to main CrI\n",
    "bdsmain = strbds_from_perc(syn_mainperc)\n",
    "spdfdict[f'dist{syn_mainperc}'] = abs((syndf.loc['Value'] - syndf.loc['True']) / \n",
    "                                      (syndf.loc[bdsmain[1]] - syndf.loc[bdsmain[0]]))\n",
    "\n",
    "# Compile and export percent CrI calculations\n",
    "synpercdf = pd.concat(spdfdict, names=['Perc', 'Run'])\n",
    "synpercdf.to_csv(f'{baserunname}_syndata_intervals.tab', sep='\\t')\n",
    "\n",
    "# Collapse Boolean df to get mean percentages within each CrI\n",
    "means = synpercdf.mean(axis=1).groupby('Perc').mean()\n",
    "means[f'Dist{syn_mainperc}Med'] = np.nanmedian(spdfdict[f'dist{syn_mainperc}'])\n",
    "means.to_csv(f'{baserunname}_syndata_means.tab', sep='\\t')\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### CALCULATE VALUES FOR SUMMARYTEXT #####\n",
    "\n",
    "# Pull values for fentanyl counterfactual ODs\n",
    "t = pd.read_csv(f'{baserunname}_final_{basescens[0][:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "nft = pd.read_csv(f'{baserunname}_final_{scenariolist[0][:-4]}_vars.tab', sep='\\t', index_col=0)\n",
    "nofentods = nft.loc['Cumulative overdose deaths', '2019']\n",
    "nofentodsdata = np.sum((t.loc['Total overdose deaths NVSS'] \n",
    "                        - t.loc['Total overdose deaths base Rx NVSS'] \n",
    "                        - t.loc['Total overdose deaths base heroin NVSS'])[::4])\n",
    "del t, nft\n",
    "\n",
    "# Calculate MCMC sample size\n",
    "mcsample = mcsettings['MCLIMIT'] - mcsettings['MCBURNIN']\n",
    "\n",
    "# Calculate PSRF percentages below 1.1 and 1.2 key thresholds\n",
    "mcout = pd.read_csv(f'{baserunname}_main_MC_MCMC_stats.tab', sep='\\t', index_col=0)\n",
    "psrfs = [i for i in mcout.index if 'PSRF' in i]\n",
    "psrfs.remove('PSRF Payoff')\n",
    "mcout = mcout.loc[psrfs]\n",
    "mcout.columns = mcout.columns.astype('float').astype('int')\n",
    "mcout = mcout[mcout.columns[mcout.columns > mcsettings['MCBURNIN']]].dropna(axis=1)\n",
    "psrf12 = np.nanmean(mcout < 1.2)\n",
    "psrf11 = np.nanmean(mcout < 1.1)\n",
    "del mcout\n",
    "\n",
    "# Get parameter numbers\n",
    "t = pd.read_csv(f'{baserunname}_params.tab', sep='\\t', index_col=0)\n",
    "iscs = len([idx for idx in t.index if 'Initial stock correction' in idx])\n",
    "estpars = len(t.index) - iscs\n",
    "del t\n",
    "\n",
    "# Compile summarytext\n",
    "summarytext = [\n",
    "    f\"Exogenous inputs\\t{len(proj_subs)}\\n\", \n",
    "    f\"Calibration time series\\t{len(fitlist) - 3}\\n\", \n",
    "    f\"MCMC total\\t{mcsettings['MCLIMIT']}\\n\", \n",
    "    f\"MCMC burnin\\t{mcsettings['MCBURNIN']}\\n\", \n",
    "    f\"MCMC sample\\t{mcsample}\\n\", \n",
    "    f\"MCMC PSRF < 1.2\\t{psrf12}\\t< 1.1\\t{psrf11}\\n\", \n",
    "    f\"Sensitivity sample\\t{int(mcsample * samplefrac)}\\n\", \n",
    "    f\"Sensitivity analysis range\\t{sensrange}\\n\", \n",
    "    f\"Syndata sets\\t{synsample}\\n\", \n",
    "    f\"Estimated parameters (no ISCs)\\t{estpars}\\n\", \n",
    "    f\"Initial stock corrections\\t{iscs}\\n\", \n",
    "    f\"Cumulative OD deaths without fentanyl\\t{int(nofentods)}\\n\", \n",
    "    f\"Cumulative synth-involved OD deaths DATA\\t{int(nofentodsdata)}\\n\", \n",
    "    f\"Confidence interval estimated params\\t{round(param_percs[-1] - param_percs[0], 3)}\\n\", \n",
    "    f\"Confidence interval estimated params\\t{syn_mainperc/100}\\n\", \n",
    "    f\"Holdout cutoff year\\t{holdoutyear}\\n\", \n",
    "    f\"Total holdout years\\t{endyear - holdoutyear}\\n\", \n",
    "    f\"Total holdout datapoints\\t{predtotal}\\n\", \n",
    "    f\"Holdout datapoints in pred. interval\\t{predright}\\n\"\n",
    "]\n",
    "\n",
    "# Calculate projection differences for key outcomes expressed as delta from 'base' to 'cnst'\n",
    "first = f'{baserunname}_sens_{basescens[0][:-4]}_clean.tab'\n",
    "second = f'{baserunname}_sens_{basescens[1][:-4]}_clean.tab'\n",
    "basecomps = compare_vals(first, second, projvars, projyear)\n",
    "\n",
    "summarytext.extend([f\"Base-Cnst delta for {var}\\t{val}\\n\" for var, val in zip(projvars, basecomps)])\n",
    "\n",
    "# Read fixed parameter values from .mdl file\n",
    "mdl = f\"../{simsettings['model']}\"\n",
    "summarytext.append(\"\\n\\nFixed parameter values\\n\")\n",
    "summarytext.extend([f'{var}\\t{get_value(mdl, var)}\\n' for var in paramvals])\n",
    "\n",
    "# Create relative Tx-seeking rate table\n",
    "ot = 1\n",
    "ob = get_value(mdl, \"Tx seeking fraction Bup Rx OUD\")\n",
    "om = round((ot - ob) * get_value(mdl, \"Tx seeking fraction MMT Rx OUD relative\"), 5)\n",
    "ov = round(ot - ob - om, 5)\n",
    "ht = get_value(mdl, \"Tx seeking rate HUD relative to Rx OUD no H\")\n",
    "hb = round(ht * get_value(mdl, \"Tx seeking fraction Bup HUD\"), 5)\n",
    "hm = round((ht - hb) * get_value(mdl, \"Tx seeking fraction MMT HUD relative\"), 5)\n",
    "hv = round(ht - hb - hm, 5)\n",
    "\n",
    "summarytext.extend([\"\\n\\n\", \"Relative Tx seeking rates\\n\", \n",
    "                    f\"OUD\\t{ot}\\t{ob}\\t{om}\\t{ov}\\n\", f\"HUD\\t{ht}\\t{hb}\\t{hm}\\t{hv}\\n\"])\n",
    "\n",
    "# Export summary text file\n",
    "with open(f\"{baserunname}_summary.txt\", 'w') as summaryfile:\n",
    "    summaryfile.writelines(summarytext)\n",
    "\n",
    "display(summarytext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SEND MAIN OUTPUTS TO SUBFOLDER FOR EASY ACCESS #####\n",
    "\n",
    "os.chdir(basedir)\n",
    "os.chdir('./Results')\n",
    "os.makedirs('./ResMain', exist_ok=True)\n",
    "\n",
    "resmain = ['assm_sens.tab', 'aldt_sens.tab', 'GOF.tab', 'inputs.tab', 'lk_sens.tab', \n",
    "           'params.tab', 'polprojann.tab', 'polprojperc.tab', 'proj_changes.tab', \n",
    "           'holdout.tab', 'syndata_means.tab', 'yearvals.tab', 'summary.txt']\n",
    "\n",
    "for res in resmain:\n",
    "    copy(f'./{baserunname}_{res}', './ResMain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcsample = pd.read_csv(f'{baserunname}_main_MC_MCMC_sample_frac.tab', sep='\\t')\n",
    "mcsample.dropna(how='all', axis=1, inplace=True)\n",
    "covs = mcsample.cov()\n",
    "corrs = mcsample.corr()\n",
    "covs.to_csv(f'{baserunname}_MC_sample_covariances.tab', sep='\\t')\n",
    "corrs.to_csv(f'{baserunname}_MC_sample_correlations.tab', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
